from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.cluster.hierarchy import linkage, fcluster, dendrogram
from scipy.spatial.distance import cdist
import os
import json
import time
from google.generativeai import GenerativeModel
from google.generativeai import types
from typing_extensions import TypedDict
from sklearn.metrics.pairwise import cosine_similarity

class ClusterTopic(TypedDict):
    topic_name: str  # 5 words max
    explanation: str  # Brief explanation of why trending
    estimated_percentage: str  # Percentage of total conversation
    key_terms: list[str]
    engagement_level: str  # High, Medium, Low
    sentiment: str  # Positive, Neutral, Negative, Mixed
    primary_language: str  # English, Spanish, Vietnamese, etc.
    user_demographics: str  # Brief description of who is discussing this topic

def filter_semantic_duplicates(df, topic_vectors, similarity_threshold=0.85):
    """
    Filter out semantically similar posts using LDA topic vectors
    
    Args:
        df: DataFrame containing the posts
        topic_vectors: LDA topic vectors for each post
        similarity_threshold: Posts with similarity above this threshold are considered duplicates
        
    Returns:
        DataFrame with semantically unique posts
    """
    # Import required modules
    import difflib
    
    # Handle empty dataframe
    if len(df) <= 1:
        return df
    
    # Fall back to text-based similarity if we have shape mismatches
    if not isinstance(topic_vectors, np.ndarray) or topic_vectors.shape[0] < len(df):
        print(f"Warning: Topic vectors shape {topic_vectors.shape[0] if isinstance(topic_vectors, np.ndarray) else 'unknown'} doesn't match dataframe size {len(df)}")
        print("Falling back to text-based similarity for deduplication")
        
        # Get texts from dataframe
        texts = df['Text'].tolist()
        indices = df.index.tolist()
        
        # Sort by engagement score to keep the highest-engagement version
        df_sorted = df.sort_values('engagement_score', ascending=False)
        texts_sorted = df_sorted['Text'].tolist()
        indices_sorted = df_sorted.index.tolist()
        
        # Track which indices to keep
        kept_indices = []
        seen_texts = set()
        
        # Use simple text similarity
        for i, text in enumerate(texts_sorted):
            idx = indices_sorted[i]
            
            # Skip exact duplicates
            if text in seen_texts:
                continue
                
            # Check similarity with seen texts
            is_duplicate = False
            for seen_text in seen_texts:
                if difflib.SequenceMatcher(None, text, seen_text).ratio() >= similarity_threshold:
                    is_duplicate = True
                    break
                    
            if not is_duplicate:
                kept_indices.append(idx)
                seen_texts.add(text)
                
        return df.loc[kept_indices]
    
    # Use LDA vectors for similarity when everything matches
    try:
        # Get indices from the dataframe
        indices = df.index.tolist()
        
        # CRITICAL FIX: Create a new array of vectors for just the current dataframe rows
        # instead of trying to use the original indices which may be out of bounds
        vectors = np.zeros((len(indices), topic_vectors.shape[1]))
        
        # Create a mapping from df index to row position in topic_vectors if possible
        index_to_pos = {}
        if hasattr(df, 'reset_index'):
            # Get the position in the original dataframe if we can
            try:
                for i, idx in enumerate(indices):
                    # Find this index's position in the original dataset
                    pos = df.index.get_loc(idx) if idx in df.index else i
                    # Only use if within bounds of topic_vectors
                    if pos < topic_vectors.shape[0]:
                        index_to_pos[idx] = pos
            except Exception as e:
                print(f"Warning: Could not map indices to positions: {e}")
                # If we can't map, we'll use fallback methods below
        
        # Safely extract topic vectors for these posts
        for i, idx in enumerate(indices):
            try:
                # First try using our mapping
                if idx in index_to_pos and index_to_pos[idx] < topic_vectors.shape[0]:
                    vectors[i] = topic_vectors[index_to_pos[idx]]
                # Next try direct indexing if index is within bounds
                elif isinstance(idx, (int, np.integer)) and idx < topic_vectors.shape[0]:
                    vectors[i] = topic_vectors[idx]
                # Use positional indexing as fallback if i is within bounds
                elif i < topic_vectors.shape[0]:
                    vectors[i] = topic_vectors[i]
                # If all else fails, use the first vector as a placeholder
                else:
                    vectors[i] = topic_vectors[0]
            except Exception as e:
                print(f"Warning: Could not get topic vector for index {idx}: {e}")
                # If direct indexing fails, use first vector as placeholder
                vectors[i] = topic_vectors[0]
                    
        # Track which indices to keep (start with the first one)
        kept_indices = [indices[0]]
        kept_vectors = [vectors[0]]
        
        # Compare each post against already kept posts
        for i in range(1, len(indices)):
            current_vector = vectors[i]
            
            # Calculate similarity with kept vectors
            similarities = []
            for kept_vector in kept_vectors:
                # For LDA vectors, simple dot product is a good similarity measure
                sim = np.dot(current_vector, kept_vector)
                similarities.append(sim)
            
            # If not too similar to any kept post, keep this one too
            if not similarities or max(similarities) < similarity_threshold:
                kept_indices.append(indices[i])
                kept_vectors.append(current_vector)
        
        # Return filtered dataframe with diverse posts
        return df.loc[kept_indices]
        
    except Exception as e:
        # If any other error occurs, fall back to simple return
        print(f"Error in semantic deduplication: {e}. Returning original dataframe.")
        return df

def lda_kmeans_clustering(recent_df):
    """
    Approach 2: LDA + K-Means Clustering
    
    This approach uses LDA for topic modeling followed by K-means clustering 
    to identify trending topics in the dataset.
    
    Args:
        recent_df: DataFrame with cleaned posts
        
    Returns:
        dict: Structured trending topics result
    """
    import os
    import numpy as np
    import pandas as pd
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
    from sklearn.decomposition import LatentDirichletAllocation
    
    print("Starting LDA + K-Means clustering approach...")
    start_time = time.time()
    
    # Create output directory if it doesn't exist
    os.makedirs('output', exist_ok=True)
    os.makedirs('output/figures', exist_ok=True)
    
    # Step 1: Use Gemini to get context-specific stopwords before doing LDA
    print("Using Gemini to identify domain-specific stopwords for better LDA results...")
    
    # Use Gemini API to identify common generic words in this specific dataset
    import os
    import json
    import google.generativeai as genai
    from google.generativeai import GenerativeModel
    from google.generativeai import types
    
    # Initial set of basic stopwords for our domain
    base_stopwords = [
        # Basic conversation words and common verbs/adjectives
        'good', 'great', 'nice', 'thank', 'thanks', 'just', 'like', 'really',
        'make', 'know', 'think', 'want', 'going', 'get', 'got', 'way', 'better',
        'im', 'dont', 'doesnt', 'time', 'day', 'today', 'tomorrow', 'yesterday',
        # Common placeholder/content-free text
        'empty_text', 'no_content', 'explained', 'explaining', 'explains', 'explanation',
    ]
    
    # Get API key from environment
    api_key = os.environ.get('GOOGLE_API_KEY')
    if not api_key:
        # Try to read from .env file as fallback
        try:
            from dotenv import load_dotenv
            load_dotenv()
            api_key = os.environ.get('GOOGLE_API_KEY')
        except ImportError:
            print("dotenv package not installed, can't load from .env file")
    
    # Sample posts to send to Gemini for stopwords identification
    import random
    sample_size = min(2000, len(recent_df))
    sampled_indices = random.sample(range(len(recent_df)), sample_size)
    sample_texts = [recent_df.iloc[idx]['cleaned_text'] for idx in sampled_indices if not pd.isna(recent_df.iloc[idx]['cleaned_text'])]
    sample_text = "\n\n".join(sample_texts[:1000])  # Use first 1000 for prompt size limitations
    
    # Configure Gemini and use it to identify domain-specific stopwords
    gemini_stopwords = []
    if api_key:
        print("Configuring Gemini to analyze common words in this dataset...")
        genai.configure(api_key=api_key)
        
        # Initialize Gemini
        model = GenerativeModel('gemini-1.5-flash')
        
        # Create a prompt to identify common meaningless words in this dataset
        prompt = f"""
        I need to identify common meaningless words in a social media dataset for text analysis.
        My goal is to filter out these words before running topic modeling (LDA).
        
        Here's a random sample of posts from the dataset:
        
        {sample_text}
        
        Please analyze this text and provide:
        
        1. A list of at least 200 common words in this dataset that:
           - Are generic and don't convey specific topics
           - Appear frequently but don't contribute to meaningful topic identification
           - Include common pronouns, prepositions, interjections, etc.
           - Include common adjectives and adverbs that don't indicate specific topics
           - Include Farcaster-specific terminology that is too general
           - Include internet slang and abbreviations
        
        2. The words should be provided in this exact format:
           ['word1', 'word2', 'word3', ...]
           So I can directly use them as a Python list
        
        IMPORTANT: Do not include subject-specific terms like 'bitcoin', 'web3', 'nft', etc. 
        I want to keep those for topic modeling. Only include truly generic terms.
        Also include versions like plurals, different tenses etc. (e.g., 'person', 'people', 'persons')
        """
        
        try:
            response = model.generate_content(prompt)
            
            # Extract the list from the response
            response_text = response.text
            
            # Find list of words in the response
            import re
            match = re.search(r'\[.*?\]', response_text, re.DOTALL)
            if match:
                try:
                    # Try to parse the extracted list as Python code
                    stopwords_list = eval(match.group(0))
                    gemini_stopwords = stopwords_list
                    print(f"Successfully extracted {len(gemini_stopwords)} dataset-specific stopwords from Gemini")
                except Exception as e:
                    print(f"Error parsing stopwords list: {e}")
            else:
                # Try to find any words in the response
                words = re.findall(r"'([^']+)'", response_text)
                if words:
                    gemini_stopwords = words
                    print(f"Extracted {len(gemini_stopwords)} stopwords using regex fallback")
                else:
                    print("Could not extract stopwords list from Gemini response")
        except Exception as e:
            print(f"Error calling Gemini API: {e}")
    else:
        print("No API key available, using default stopwords list")
    
    # Combine with our base stopwords
    print("Adding dataset-specific stopwords to base list...")
    additional_stopwords = base_stopwords + gemini_stopwords
    
    # Remove duplicates and ensure all words are lowercase
    additional_stopwords = list(set([word.lower() for word in additional_stopwords]))
    
    print(f"Final stopwords list contains {len(additional_stopwords)} words")
    
    # Now proceed with text vectorization using enhanced stopwords
    print("Creating document-term matrix with optimized settings...")
    
    # For LDA, we need count vectors (not TF-IDF)
    max_features = 7500  # Increase features for better topic modeling
    
    # Create final extended stopwords list
    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
    extended_stop_words = list(ENGLISH_STOP_WORDS) + additional_stopwords
    
    # Create vectorizer with improved settings for better topic modeling
    vectorizer = CountVectorizer(
        max_features=max_features,
        stop_words=extended_stop_words,
        min_df=10,  # Term must appear in at least 10 documents (increased threshold)
        max_df=0.70,  # Filter out terms that appear in >70% of documents (more aggressive filtering)
        ngram_range=(1, 2),  # Include bigrams for better topic detection
        token_pattern=r'\b[a-zA-Z][a-zA-Z]{2,}\b'  # Only words with at least 3 letters
    )
    
    # Handle None values in cleaned_text
    recent_df['cleaned_text'] = recent_df['cleaned_text'].fillna("empty_text")
    
    print(f"Processing {len(recent_df['cleaned_text'])} documents with {max_features} features...")
    X = vectorizer.fit_transform(recent_df['cleaned_text'])
    feature_names = vectorizer.get_feature_names_out()
    
    print(f"Created document-term matrix with {X.shape[0]} documents and {X.shape[1]} features")
    
    # Step 2: LDA Topic Modeling with optimal parameters
    print("Fitting LDA model with optimized processing...")
    import multiprocessing
    n_jobs = multiprocessing.cpu_count() - 1  # Use all cores except one
    print(f"Using {n_jobs} CPU cores for parallel processing")
    
    n_topics = 20  # Number of topics to model
    
    # Try to use CuML CUDA implementation if available
    try:
        from cuml.decomposition import LatentDirichletAllocation as CuLDA
        # Check if enough GPU memory is available (rough estimate)
        import cupy as cp
        free_mem = cp.cuda.runtime.memGetInfo()[0]
        required_mem = X.shape[0] * X.shape[1] * 8 * 2  # Rough estimate
        
        if free_mem > required_mem:
            print("Using CUDA-accelerated LDA implementation")
            # Convert to CuPy array
            X_gpu = cp.sparse.csr_matrix(X)
            lda = CuLDA(
                n_components=n_topics,
                random_state=42,
                max_iter=25,
                verbose=True
            )
            lda_output = lda.fit_transform(X_gpu)
            # Convert back to numpy
            lda_output = cp.asnumpy(lda_output)
            is_cuml = True
        else:
            raise ImportError("Not enough GPU memory")
            
    except ImportError as e:
        print(f"CUDA LDA not available: {e}. Using CPU implementation.")
        # For CPU implementation, use large batch size and efficient settings
        # Use aggressively large batch size to take advantage of RAM
        batch_size = max(10000, X.shape[0] // 10)  # 10% of data or 10k minimum
        print(f"Using large batch size of {batch_size} for better memory utilization")
        
        lda = LatentDirichletAllocation(
            n_components=n_topics,
            random_state=42,
            max_iter=30,  # Increased iterations for better convergence
            n_jobs=n_jobs,  # Utilize multiple CPU cores
            learning_method='batch',  # Use batch learning for better parallelization
            batch_size=batch_size,  # Much larger batch size for better memory utilization
            verbose=1,  # Show progress
            # LDA hyperparameters for better topic quality
            doc_topic_prior=0.3,  # Slightly lower alpha for more specific topic mixtures (default is 1.0)
            topic_word_prior=0.1,  # Lower beta for more focused topics (default is 1.0)
            perp_tol=0.1  # Less strict tolerance for faster convergence
        )
        
        # Fit and transform
        print("Starting LDA fit_transform - this may take a while...")
        lda_output = lda.fit_transform(X)
        is_cuml = False
    
    # Get topic-term distributions
    print("Extracting topic keywords...")
    topic_keywords = []
    
    # Define a function to filter out meaningless terms
    def is_meaningful_term(term):
        # Skip very short terms
        if len(term) < 3:
            return False
            
        # Keep special terms with $ or @ symbols (cryptocurrencies, mentions)
        if '$' in term or '@' in term or '#' in term:
            return True
            
        # Filter out terms that are mostly digits
        if sum(c.isdigit() for c in term) > len(term) // 2:
            return False
            
        # Keep domain-specific meaningful terms even if short
        domain_terms = {'nft', 'eth', 'btc', 'dao', 'defi', 'web3', 'ai', 'ml'}
        if term.lower() in domain_terms:
            return True
            
        return True
    
    # Process each topic to extract keywords
    for topic_idx, topic in enumerate(lda.components_):
        # Get top terms for this topic (get more than we need for filtering)
        top_terms_idx = topic.argsort()[:-50-1:-1]
        
        # Filter the terms to only meaningful ones
        filtered_terms = []
        for idx in top_terms_idx:
            term = feature_names[idx]
            if is_meaningful_term(term):
                filtered_terms.append(term)
                
            if len(filtered_terms) >= 15:  # Get up to 15 terms
                break
                
        # Make sure we have enough terms (at least 10)
        if len(filtered_terms) < 10:
            # If we don't have enough terms after filtering, get the top ones unfiltered
            filtered_terms = [feature_names[idx] for idx in top_terms_idx[:10]]
            
        # Take the top 10 terms for this topic
        topic_keywords.append(filtered_terms[:10])
            "topics": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "topic_name": {"type": "string", "description": "A concise name for the topic (3-5 words max)"},
                        "keywords": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "10-15 specific, meaningful keywords that define this topic"
                        },
                        "description": {"type": "string", "description": "A brief description of what this topic represents"}
                    },
                    "required": ["topic_name", "keywords", "description"]
                }
            },
            "num_topics": {"type": "integer", "description": "The total number of topics identified"}
        },
        "required": ["topics", "num_topics"]
    }
    
    # Prepare a random sample of documents to send to Gemini
    print("Preparing document sample for Gemini analysis...")
    import random
    
    # Get a more diverse and representative sample (up to 5000 documents for context window limits)
    sample_size = min(5000, len(recent_df))
    sampled_indices = random.sample(range(len(recent_df)), sample_size)
    sample_texts = []
    
    # Include engagement metrics with texts for better topic identification
    for idx in sampled_indices:
        row = recent_df.iloc[idx]
        likes = 0 if pd.isna(row.get('likes_count', 0)) else int(row.get('likes_count', 0))
        recasts = 0 if pd.isna(row.get('recasts_count', 0)) else int(row.get('recasts_count', 0))
        formatted_text = f"[ðŸ‘{likes}|â†—ï¸{recasts}]: {row.get('cleaned_text', '')}"
        sample_texts.append(formatted_text)
    
    # Join the texts and create the prompt
    joined_texts = "\n\n".join(sample_texts)
    
    # Get date range for context
    try:
        date_range = f"{recent_df['datetime'].min().strftime('%Y-%m-%d')} to {recent_df['datetime'].max().strftime('%Y-%m-%d')}"
    except:
        date_range = "the analyzed period"
    
    prompt = f"""
    You are a topic modeling expert. I need you to analyze a large collection of social media posts from Farcaster
    during the period: {date_range}.
    
    These posts have engagement metrics in the format [ðŸ‘X|â†—ï¸Y] where X is the number of likes and Y is the number of recasts.
    Higher engagement suggests more popular topics.
    
    Your task is to:
    1. Identify 8-12 distinct, meaningful topics in these posts
    2. For each topic, provide:
       - A concise topic name (3-5 words)
       - 10-15 specific keywords that define this topic
       - A brief description of what this topic represents
    
    IMPORTANT GUIDELINES:
    - Focus on SPECIFIC topics rather than generic ones (e.g., "Base Chain DeFi Projects" is better than just "Cryptocurrency")
    - Avoid topics centered around common words like "people", "things", "new", etc.
    - Prioritize topics that appear to have higher engagement (more likes and recasts)
    - Identify concrete topics, not just linguistic patterns
    - Make sure topics are distinct from each other
    - Keywords should be specific and meaningful, avoiding generic terms
    
    POSTS:
    {joined_texts}
    """
    
    # Call Gemini to get structured topics
    print("Calling Gemini API for topic extraction...")
    try:
        response = model.generate_content(
            prompt,
            generation_config=types.GenerationConfig(
                temperature=0.2,
                response_mime_type="application/json",
                schema=topic_schema
            )
        )
        
        # Parse the response
        try:
            gemini_topics = json.loads(response.text)
            print(f"Successfully received {gemini_topics.get('num_topics', 0)} topics from Gemini")
            
            # Process the topics into our expected format
            topic_keywords = []
            for topic in gemini_topics['topics']:
                # Get the keywords for this topic
                keywords = topic['keywords'][:10]  # Take up to 10 keywords
                topic_keywords.append(keywords)
                
                # Print the topic details
                print(f"Topic: {topic['topic_name']}")
                print(f"Keywords: {', '.join(keywords)}")
                print(f"Description: {topic['description']}")
                print()
            
        except json.JSONDecodeError as e:
            print(f"Error parsing JSON response: {e}")
            # Fallback to simple topics if parsing fails
            topic_keywords = [
                ["base", "chain", "blockchain", "transaction", "ethereum", "network", "crypto", "bridge", "protocol", "mainnet"],
                ["nft", "art", "collection", "mint", "artist", "creators", "digital", "ownership", "token", "gallery"],
                ["defi", "tokens", "liquidity", "swap", "trading", "yield", "staking", "finance", "investment", "market"],
                ["social", "community", "engagement", "friends", "creator", "followers", "network", "conversation", "interact", "content"],
                ["gaming", "game", "play", "level", "score", "player", "reward", "competition", "challenge", "achievement"],
                ["airdrop", "rewards", "points", "distribution", "claim", "allocation", "free", "incentive", "token", "launch"]
            ]
    except Exception as e:
        print(f"Error calling Gemini API: {e}")
        # Fallback to simple topics if API fails
        topic_keywords = [
            ["base", "chain", "blockchain", "transaction", "ethereum", "network", "crypto", "bridge", "protocol", "mainnet"],
            ["nft", "art", "collection", "mint", "artist", "creators", "digital", "ownership", "token", "gallery"],
            ["defi", "tokens", "liquidity", "swap", "trading", "yield", "staking", "finance", "investment", "market"],
            ["social", "community", "engagement", "friends", "creator", "followers", "network", "conversation", "interact", "content"],
            ["gaming", "game", "play", "level", "score", "player", "reward", "competition", "challenge", "achievement"],
            ["airdrop", "rewards", "points", "distribution", "claim", "allocation", "free", "incentive", "token", "launch"]
        ]
    
    # Create output structure compatible with the rest of the pipeline
    n_topics = len(topic_keywords)
    
    # Prepare a compatibility layer for the K-means step
    # Build a synthetic topic-document matrix (similar to what LDA would produce)
    # We'll generate a random distribution that roughly aligns with our topics
    import numpy as np
    
    print("Creating synthetic topic-document matrix from Gemini topics...")
    
    # Create a document-topic matrix with random assignments weighted by token presence
    lda_output = np.zeros((len(recent_df), n_topics))
    
    # Vectorize the documents to check for keyword presence
    from sklearn.feature_extraction.text import CountVectorizer
    
    # Flatten all keywords into a single vocabulary
    all_keywords = [keyword for topic in topic_keywords for keyword in topic]
    
    # Create a vectorizer for our keyword vocabulary
    keyword_vectorizer = CountVectorizer(vocabulary=all_keywords)
    
    # Transform documents to get keyword counts
    keyword_counts = keyword_vectorizer.fit_transform(recent_df['cleaned_text'].fillna(''))
    
    # Convert to a dense array
    keyword_counts_dense = keyword_counts.toarray()
    
    # Create a mapping from keyword to topic index
    keyword_to_topic = {}
    for topic_idx, keywords in enumerate(topic_keywords):
        for keyword in keywords:
            keyword_to_topic[keyword] = topic_idx
    
    # For each document, assign topic probabilities based on keyword presence
    for doc_idx in range(len(recent_df)):
        # Get counts for all keywords in this document
        doc_keyword_counts = keyword_counts_dense[doc_idx]
        
        # Calculate topic affinities based on keyword presence
        topic_affinities = np.zeros(n_topics)
        
        for keyword_idx, count in enumerate(doc_keyword_counts):
            if count > 0:
                keyword = all_keywords[keyword_idx]
                topic_idx = keyword_to_topic.get(keyword, -1)
                if topic_idx >= 0:
                    topic_affinities[topic_idx] += count
        
        # If no keywords matched, assign uniform distribution
        if np.sum(topic_affinities) == 0:
            topic_affinities = np.ones(n_topics) / n_topics
        else:
            # Normalize to get probabilities
            topic_affinities = topic_affinities / np.sum(topic_affinities)
        
        # Assign to LDA output
        lda_output[doc_idx] = topic_affinities
    
    # Ensure the matrix is properly normalized
    row_sums = lda_output.sum(axis=1)
    lda_output = lda_output / row_sums[:, np.newaxis]
    
    # Flag that we're using Gemini instead of LDA/cuML
    is_cuml = False
    
    # Optimized silhouette score calculation by sampling
    if n_topics > 1:
        print("Calculating silhouette score with sampling...")
        # Use sampling to make silhouette calculation feasible
        sample_size = min(10000, X.shape[0])
        # Create random sample indices
        sample_indices = np.random.choice(X.shape[0], size=sample_size, replace=False)
        
        # Use sampled subset for silhouette calculation
        try:
            silhouette_avg = silhouette_score(
                X[sample_indices], 
                lda_output.argmax(axis=1)[sample_indices],
                sample_size=min(5000, sample_size)  # Further sampling for faster calculation
            )
            print(f"Silhouette Score (from {sample_size} samples): {silhouette_avg:.3f}")
        except Exception as e:
            print(f"Silhouette score calculation failed: {e}")
            print("Continuing without silhouette score")
    
    # Use CPU implementation for similarity calculation
    print("Using scikit-learn for similarity calculation")
    topic_similarity = cosine_similarity(lda.components_)
    
    # Visualize topic similarity with enhanced Seaborn styling
    plt.figure(figsize=(14, 12))
    sns.set(font_scale=1.1)
    mask = np.triu(np.ones_like(topic_similarity, dtype=bool))  # Create mask for upper triangle
    with sns.axes_style("white"):
        ax = sns.heatmap(
            topic_similarity, 
            annot=True, 
            cmap="YlGnBu", 
            fmt=".2f",
            linewidths=0.5,
            mask=mask,  # Only show lower triangle to reduce redundancy
            cbar_kws={'label': 'Cosine Similarity'}
        )
    plt.title('Topic Similarity Matrix', fontsize=16, fontweight='bold', pad=20)
    plt.tight_layout()
    plt.savefig('output/figures/topic_similarity_matrix.png', dpi=300)
    
    # Hierarchical clustering of topics based on similarity
    # This will help identify groups of similar topics
    topic_linkage = linkage(1 - topic_similarity, method='ward')
    
    # Plot dendrogram to visualize topic clusters
    plt.figure(figsize=(14, 7))
    
    def fancy_dendrogram(*args, **kwargs):
        max_d = kwargs.pop('max_d', None)
        if max_d and 'color_threshold' not in kwargs:
            kwargs['color_threshold'] = max_d
        plt.figure(figsize=(12, 8))
        ddata = dendrogram(*args, **kwargs)
        if max_d:
            plt.axhline(y=max_d, c='k', ls='--', lw=1)
        return ddata
    
    fancy_dendrogram(
        topic_linkage,
        labels=[f"Topic {i+1}" for i in range(n_topics)],
        leaf_font_size=10,
        orientation='right'
    )
    plt.title('Hierarchical Clustering of Topics')
    plt.xlabel('Distance')
    plt.savefig('output/figures/topic_clustering_dendrogram.png')
    
    # Determine optimal number of topic clusters
    topic_cluster_threshold = 0.7  # Similarity threshold
    topic_clusters = fcluster(topic_linkage, t=topic_cluster_threshold, criterion='distance')
    
    # Create mapping of original topics to consolidated topics
    topic_mapping = {}
    consolidated_topic_keywords = []
    
    # For each cluster, find the most representative topic
    for cluster_id in range(1, max(topic_clusters) + 1):
        # Get all topics in this cluster
        cluster_topic_indices = [i for i, c in enumerate(topic_clusters) if c == cluster_id]
        
        # If cluster has only one topic, keep it as is
        if len(cluster_topic_indices) == 1:
            topic_idx = cluster_topic_indices[0]
            topic_mapping[topic_idx] = len(consolidated_topic_keywords)
            consolidated_topic_keywords.append(topic_keywords[topic_idx])
        else:
            # Find the topic with highest topic coherence (using average document probability as proxy)
            topic_coherence = [lda_output[:, idx].mean() for idx in cluster_topic_indices]
            representative_topic_idx = cluster_topic_indices[np.argmax(topic_coherence)]
            
            # For all topics in cluster, map to the representative topic
            for topic_idx in cluster_topic_indices:
                topic_mapping[topic_idx] = len(consolidated_topic_keywords)
                
            # Merge keywords from all topics in cluster
            merged_keywords = []
            for topic_idx in cluster_topic_indices:
                merged_keywords.extend(topic_keywords[topic_idx])
            
            # Keep unique keywords, prioritizing those from the representative topic
            unique_merged_keywords = []
            seen = set()
            
            # First add keywords from representative topic
            for keyword in topic_keywords[representative_topic_idx]:
                if keyword not in seen:
                    unique_merged_keywords.append(keyword)
                    seen.add(keyword)
            
            # Then add unique keywords from other topics
            for keyword in merged_keywords:
                if keyword not in seen and len(unique_merged_keywords) < 15:
                    unique_merged_keywords.append(keyword)
                    seen.add(keyword)
            
            consolidated_topic_keywords.append(unique_merged_keywords)
    
    print(f"Reduced {n_topics} original topics to {len(consolidated_topic_keywords)} consolidated topics")
    
    # Map documents to consolidated topics
    document_consolidated_topics = np.zeros((lda_output.shape[0], len(consolidated_topic_keywords)))
    
    for doc_idx in range(lda_output.shape[0]):
        for orig_topic_idx in range(n_topics):
            consolidated_idx = topic_mapping[orig_topic_idx]
            document_consolidated_topics[doc_idx, consolidated_idx] += lda_output[doc_idx, orig_topic_idx]
    
    # Normalize to ensure probabilities sum to 1
    document_consolidated_topics = document_consolidated_topics / document_consolidated_topics.sum(axis=1, keepdims=True)
    
    # Replace original LDA output with consolidated version
    lda_output = document_consolidated_topics
    topic_keywords = consolidated_topic_keywords
    n_consolidated_topics = len(consolidated_topic_keywords)
    
    print(f"Consolidated topics and their top keywords:")
    for i, keywords in enumerate(consolidated_topic_keywords):
        print(f"Topic {i+1}: {', '.join(keywords[:10])}")
    
    # Step 3: K-Means Clustering on Consolidated LDA Results
    print("Performing K-Means clustering on LDA output with parallel processing...")
    
    # Determine optimal number of clusters
    # We'll try a range of clusters and pick the one with best silhouette score
    silhouette_scores = []
    K_range = range(3, 8)  # Try between 3 and 7 clusters
    results = []
    
    # Use scikit-learn for silhouette score and KMeans
    print("Using scikit-learn's silhouette_score")
    silhouette_func = silhouette_score
    
    # Run K-means for each K value
    print("Using scikit-learn's KMeans")
    
    # Optimize for large datasets by sampling to enable dynamic K selection
    print("Using sampling for silhouette score calculation")
    # Create a smaller sample for silhouette calculation
    sample_size = min(10000, lda_output.shape[0])
    sample_indices = np.random.choice(lda_output.shape[0], size=sample_size, replace=False)
    lda_output_sample = lda_output[sample_indices]
    
    # Check for GPU availability
    try:
        import cupy as cp
        use_gpu = True
        print("Using GPU acceleration with CuPy")
    except ImportError:
        use_gpu = False
        print("CuPy not available - using CPU implementation")
    
    # Use large batch sizes to leverage available memory
    for k in K_range:
        print(f"Testing K={k}...")
        # Use MiniBatchKMeans for improved memory and speed
        from sklearn.cluster import MiniBatchKMeans
        batch_size = min(10000, lda_output.shape[0] // 10)  # 10% of data or 10k max
        
        kmeans = MiniBatchKMeans(
            n_clusters=k, 
            random_state=42,
            batch_size=batch_size,
            n_init=3,
            verbose=1
        )
        
        # First fit full data
        print(f"Fitting MiniBatchKMeans with batch size {batch_size}...")
        clusters = kmeans.fit_predict(lda_output)
        
        # Then calculate silhouette score on sample
        try:
            clusters_sample = kmeans.predict(lda_output_sample)
            score = silhouette_func(
                lda_output_sample, 
                clusters_sample,
                sample_size=min(5000, sample_size)  # Further sampling for speed
            )
            print(f"K={k}, Silhouette Score (sampled): {score:.3f}")
        except Exception as e:
            print(f"Silhouette calculation failed: {e}")
            score = 0.5 - abs(k - 5) * 0.1  # Fallback heuristic (favor k=5)
            print(f"Using fallback score: {score:.3f}")
            
        silhouette_scores.append(score)
        results.append((k, score, clusters, kmeans))
    
    # Scores are already stored in silhouette_scores
    
    # Plot silhouette scores with Seaborn
    plt.figure(figsize=(10, 6))
    sns.set_style("whitegrid")
    sns.lineplot(x=list(K_range), y=silhouette_scores, marker='o', color='royalblue', linewidth=2.5)
    plt.xlabel('Number of Clusters (K)', fontsize=12)
    plt.ylabel('Silhouette Score', fontsize=12)
    plt.title('Optimal Number of Clusters', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig('output/figures/optimal_k_clusters.png', dpi=300)
    
    # Find optimal K (highest silhouette score)
    best_idx = np.argmax(silhouette_scores)
    optimal_k = K_range[best_idx]
    print(f"Optimal number of clusters: {optimal_k}")
    
    # Get the best KMeans and clusters from our parallel runs
    _, _, clusters, kmeans = results[best_idx]
    
    # Assign cluster to each document
    recent_df['lda_cluster'] = clusters
    
    # Get cluster centers and dominant topics
    cluster_centers = kmeans.cluster_centers_
    
    # Calculate cluster sizes
    cluster_sizes = [(clusters == i).sum() for i in range(len(cluster_centers))]
    
    # Identify the dominant LDA topics for each cluster
    dominant_topics_per_cluster = []
    for i, center in enumerate(cluster_centers):
        # Get top 3 LDA topics for this cluster
        top_topic_idx = center.argsort()[::-1][:3]
        top_topic_weights = center[top_topic_idx]
        
        cluster_topics = []
        for idx, weight in zip(top_topic_idx, top_topic_weights):
            cluster_topics.append({
                'topic_idx': int(idx),
                'weight': float(weight),
                'keywords': topic_keywords[idx][:10]
            })
            
        dominant_topics_per_cluster.append({
            'cluster_id': i,
            'dominant_topics': cluster_topics,
            'size': int((clusters == i).sum())
        })
        
        print(f"Cluster {i} (size: {(clusters == i).sum()}):")
        for topic in cluster_topics:
            print(f"  Topic {topic['topic_idx']} (weight: {topic['weight']:.3f}): {', '.join(topic['keywords'])}")
    
    # Visualize cluster distribution with Seaborn
    plt.figure(figsize=(10, 6))
    sns.set_style("whitegrid")
    ax = sns.barplot(
        x=list(range(len(cluster_sizes))), 
        y=cluster_sizes,
        palette="viridis"
    )
    # Add value labels on top of each bar
    for i, v in enumerate(cluster_sizes):
        ax.text(i, v + 0.1, str(v), ha='center', fontsize=10)
    plt.xlabel('Cluster ID', fontsize=12)
    plt.ylabel('Number of Documents', fontsize=12)
    plt.title('Documents per Cluster', fontsize=14, fontweight='bold')
    plt.xticks(range(len(cluster_sizes)))
    plt.tight_layout()
    plt.savefig('output/figures/cluster_distribution.png', dpi=300)
    
    # Find exemplar documents for each cluster (closest to cluster center)
    exemplars = {}
    for cluster_id in range(len(cluster_centers)):
        # Get documents in this cluster
        cluster_docs_idx = np.where(clusters == cluster_id)[0]
        
        # Skip empty clusters
        if len(cluster_docs_idx) == 0:
            exemplars[cluster_id] = []
            continue
            
        # Calculate distance to cluster center
        cluster_docs_vectors = lda_output[cluster_docs_idx]
        distances = cdist(cluster_docs_vectors, [cluster_centers[cluster_id]], 'euclidean').flatten()
        
        # Get indices of 15 closest documents as candidates (for semantic filtering)
        closest_indices = np.argsort(distances)[:min(15, len(distances))]
        exemplar_indices = cluster_docs_idx[closest_indices]
        
        # Get the actual documents
        exemplar_candidates = recent_df.iloc[exemplar_indices]
        
        # Create a temporary dataframe for semantic filtering
        # Import pandas here to prevent UnboundLocalError
        import pandas as pd
        exemplar_candidates_with_vectors = pd.DataFrame({
            'index': exemplar_indices,
            'text': exemplar_candidates['Text'].values
        })
        
        # Apply semantic filtering using LDA vectors
        # Get vectors for candidates
        candidate_vectors = lda_output[exemplar_indices]
        
        # Track which indices to keep (start with the closest one)
        kept_indices = [0]  # Start with the closest one
        kept_vectors = [candidate_vectors[0]]
        
        # Compare each document against already kept documents
        for i in range(1, len(candidate_vectors)):
            current_vector = candidate_vectors[i]
            
            # Calculate similarity with kept documents
            similarities = []
            for kept_vector in kept_vectors:
                # For LDA vectors, cosine similarity is appropriate
                sim = np.dot(current_vector, kept_vector)
                similarities.append(sim)
            
            # If not too similar to any kept document, keep this one too
            if not similarities or max(similarities) < 0.85:
                kept_indices.append(i)
                kept_vectors.append(current_vector)
            
            # Stop once we have 5 unique documents
            if len(kept_indices) >= 5:
                break
        
        # Get the final exemplar indices and documents
        final_exemplar_indices = [exemplar_indices[i] for i in kept_indices]
        exemplar_docs = recent_df.iloc[final_exemplar_indices]
        
        # Store the results
        exemplars[cluster_id] = exemplar_docs[['Text', 'engagement_score']].to_dict('records')
        
        print(f"Exemplar documents for Cluster {cluster_id}:")
        for doc in exemplars[cluster_id][:2]:  # Just print 2 to save space
            print(f"  - {doc['Text'][:100]}..." if len(doc['Text']) > 100 else f"  - {doc['Text']}")
    
    # Step 4: Gemini Labeling of Clusters with Structured Response
    print("Using Gemini to label clusters...")
    
    # For each cluster, get representative texts
    cluster_data = []
    for cluster_id in range(len(cluster_centers)):
        # Get documents in this cluster
        cluster_docs = recent_df[recent_df['lda_cluster'] == cluster_id]
        
        # Get top keywords for this cluster
        top_topic_idx = np.argmax(cluster_centers[cluster_id])
        keywords = topic_keywords[top_topic_idx]
        
        # Calculate engagement metrics for this cluster
        total_engagement = cluster_docs['engagement_score'].sum()
        avg_engagement = cluster_docs['engagement_score'].mean()
        max_engagement = cluster_docs['engagement_score'].max()
        
        # Calculate trending and recency scores similar to approach3
        trending_score = 1.0
        recency_score = 1.0
        
        # Calculate time-based metrics to determine if the cluster is trending
        if 'datetime' in cluster_docs.columns:
            # Get the timestamps and sort them
            timestamps = pd.to_datetime(cluster_docs['datetime']).sort_values()
            
            # Get recency information (how recent is the latest activity)
            latest_time = timestamps.max()
            max_possible_time = pd.to_datetime(recent_df['datetime']).max()
            hours_ago = (max_possible_time - latest_time).total_seconds() / 3600
            
            # Convert to a recency score (higher is better/more recent)
            recency_score = np.exp(-hours_ago / 24.0)  # Decay by e^(-hours/24)
            
            # If we have enough data points, calculate the trending score
            if len(timestamps) >= 3:
                # Calculate posts per hour in the first third vs last third of the time period
                time_range = (timestamps.max() - timestamps.min()).total_seconds() / 3600
                if time_range > 0:
                    # Split into three equal time periods
                    time_split1 = timestamps.min() + (timestamps.max() - timestamps.min()) / 3
                    time_split2 = timestamps.min() + 2 * (timestamps.max() - timestamps.min()) / 3
                    
                    # Count posts in each period
                    early_posts = sum(timestamps < time_split1)
                    middle_posts = sum((timestamps >= time_split1) & (timestamps < time_split2))
                    recent_posts = sum(timestamps >= time_split2)
                    
                    # Get first and last third durations
                    early_period = max(0.1, (time_split1 - timestamps.min()).total_seconds() / 3600)
                    recent_period = max(0.1, (timestamps.max() - time_split2).total_seconds() / 3600)
                    
                    # Calculate posts per hour for each period
                    early_rate = early_posts / early_period
                    recent_rate = recent_posts / recent_period
                    
                    # Trending score: ratio of recent to early post rate
                    trending_score = recent_rate / max(0.1, early_rate) if early_rate > 0 else 1.0
        
        # Calculate a combined score (similar to approach3)
        engagement_weight = 0.4
        trending_weight = 0.3
        recency_weight = 0.3
        
        # Calculate the weighted trending score
        combined_score = (
            engagement_weight * total_engagement + 
            trending_weight * trending_score * total_engagement +
            recency_weight * recency_score * total_engagement
        )
        
        # Get a broader range of samples to leverage Gemini's large context window
        # First get high-engagement samples (top 25%) - get more than needed for filtering
        engagement_high_threshold = cluster_docs['engagement_score'].quantile(0.75)
        high_engaged_docs = cluster_docs[cluster_docs['engagement_score'] >= engagement_high_threshold]
        
        # Also get some medium engagement posts (between 25% and 75%)
        engagement_med_threshold = cluster_docs['engagement_score'].quantile(0.25)
        med_engaged_docs = cluster_docs[(cluster_docs['engagement_score'] < engagement_high_threshold) & 
                                    (cluster_docs['engagement_score'] >= engagement_med_threshold)]
        
        # Get some from lower engagement too for diversity (bottom 25%)
        low_engaged_docs = cluster_docs[cluster_docs['engagement_score'] < engagement_med_threshold]
        
        # Calculate target sample sizes (will be reduced after deduplication)
        high_target_size = min(150, len(high_engaged_docs))  # Get more candidates for filtering
        med_target_size = min(75, len(med_engaged_docs))
        low_target_size = min(40, len(low_engaged_docs))
        
        # Ensure we have at least some samples even if distribution is skewed
        if high_target_size < 10 and len(cluster_docs) >= 10:
            high_engaged_docs = cluster_docs.nlargest(min(10, len(cluster_docs)), 'engagement_score')
            high_target_size = len(high_engaged_docs)
        
        # Sample from each engagement level
        high_candidates = high_engaged_docs.sample(n=high_target_size) if high_target_size > 0 and len(high_engaged_docs) > 0 else pd.DataFrame()
        med_candidates = med_engaged_docs.sample(n=med_target_size) if med_target_size > 0 and len(med_engaged_docs) > 0 else pd.DataFrame() 
        low_candidates = low_engaged_docs.sample(n=low_target_size) if low_target_size > 0 and len(low_engaged_docs) > 0 else pd.DataFrame()
        
        # Apply semantic filtering to each group
        high_filtered = filter_semantic_duplicates(
            high_candidates, 
            lda_output,
            similarity_threshold=0.85
        ) if not high_candidates.empty else pd.DataFrame()
        
        # Use different similarity thresholds for different engagement levels
        # High similarity threshold = more filtering
        med_filtered = filter_semantic_duplicates(
            med_candidates, 
            lda_output,
            similarity_threshold=0.80
        ) if not med_candidates.empty else pd.DataFrame()
        
        low_filtered = filter_semantic_duplicates(
            low_candidates, 
            lda_output,
            similarity_threshold=0.75
        ) if not low_candidates.empty else pd.DataFrame()
        
        # Combine filtered samples
        filtered_samples = pd.concat([high_filtered, med_filtered, low_filtered])
        
        # Also add some exemplar documents (closest to cluster center)
        cluster_docs_vectors = lda_output[recent_df.index.isin(cluster_docs.index)]
        if len(cluster_docs_vectors) > 0:
            distances = cdist(cluster_docs_vectors, [cluster_centers[cluster_id]], 'euclidean').flatten()
            closest_indices = np.argsort(distances)[:50]  # Get 50 closest to center as candidates
            exemplar_indices = cluster_docs.iloc[closest_indices].index if len(closest_indices) > 0 else []
            exemplar_candidates = recent_df.loc[exemplar_indices] if len(exemplar_indices) > 0 else pd.DataFrame()
            
            # Filter exemplars semantically
            if not exemplar_candidates.empty:
                exemplar_filtered = filter_semantic_duplicates(
                    exemplar_candidates, 
                    lda_output,
                    similarity_threshold=0.85
                ).head(25)  # Keep at most 25 exemplars
                
                # Add exemplars to samples if they exist
                if not exemplar_filtered.empty:
                    # Combine with existing samples but filter again to remove duplicates across categories
                    combined_candidates = pd.concat([filtered_samples, exemplar_filtered])
                    # Final filtering across all categories
                    sampled_docs = filter_semantic_duplicates(
                        combined_candidates, 
                        lda_output,
                        similarity_threshold=0.85  # High threshold for final filtering
                    )
                else:
                    sampled_docs = filtered_samples
            else:
                sampled_docs = filtered_samples
        else:
            sampled_docs = filtered_samples
        
        # Get sample texts with engagement metrics
        import pandas as pd
        sample_texts_with_engagement = sampled_docs.apply(
            lambda row: f"[ðŸ‘{0 if pd.isna(row.get('likes_count', 0)) else int(row.get('likes_count', 0))}|â†—ï¸{0 if pd.isna(row.get('recasts_count', 0)) else int(row.get('recasts_count', 0))}]: {row.get('Text', '')}", 
            axis=1
        ).tolist()
        
        # Format hours ago for display
        hours_ago_str = ""
        if 'datetime' in cluster_docs.columns:
            hours_ago_str = f"{hours_ago:.1f}h ago"
            
        # Add trending metrics to log
        print(f"Cluster {cluster_id}: {len(cluster_docs)} posts, {avg_engagement:.1f} avg engagement, {trending_score:.2f} trending, {recency_score:.2f} recency ({hours_ago_str}), {combined_score:.1f} score")
            
        cluster_data.append({
            'cluster_id': cluster_id,
            'size': len(cluster_docs),
            'keywords': keywords,
            'sample_texts': sample_texts_with_engagement,
            'total_engagement': total_engagement,
            'avg_engagement': avg_engagement,
            'max_engagement': max_engagement,
            'trending_score': trending_score,
            'recency_score': recency_score,
            'combined_score': combined_score,
            'hours_ago': hours_ago if 'datetime' in cluster_docs.columns else 0
        })
    
    # Use Gemini to label each cluster
    cluster_topics = []
    
    # Initialize Gemini with API key
    import os
    import google.generativeai as genai
    
    api_key = os.environ.get('GOOGLE_API_KEY')
    if not api_key:
        # Try to read from .env file as fallback
        try:
            from dotenv import load_dotenv
            load_dotenv()
            api_key = os.environ.get('GOOGLE_API_KEY')
        except ImportError:
            print("dotenv package not installed, can't load from .env file")
    
    # Configure Gemini with API key        
    if api_key:
        print(f"Configuring Gemini with API key from environment")
        genai.configure(api_key=api_key)
    else:
        print("WARNING: No GOOGLE_API_KEY found in environment or .env file")
        print("Gemini might not work without an API key")
    
    model = GenerativeModel('gemini-2.0-flash-lite')
    
    for cluster in cluster_data:
        # Create structured prompt
        # Calculate sample distribution stats from cluster size
        cluster_id = cluster['cluster_id']
        total_posts = cluster['size']
        
        # Approximate the engagement distribution based on typical distributions
        # since we don't have access to the original high/med/low counts here
        high_engagement_count = int(total_posts * 0.25)  # ~25% high engagement
        med_engagement_count = int(total_posts * 0.50)   # ~50% medium engagement
        low_engagement_count = total_posts - high_engagement_count - med_engagement_count  # ~25% low engagement
        
        # Create a more comprehensive prompt leveraging Gemini 2.0's large context window
        prompt = f"""
        I need to identify the single most specific TRENDING topic being discussed in a cluster of Farcaster social media posts.
        
        A TRENDING topic has these characteristics:
        - HIGH ENGAGEMENT: Topics with many likes and recasts
        - RECENCY: Topics that are active in the most recent timeframe
        - GROWTH: Topics that show increasing activity over time
        - CONVERSATION: Topics that generate many replies and discussions
        
        KEY INFORMATION ABOUT THIS CLUSTER:
        - Top keywords: {', '.join(cluster['keywords'])}
        - Cluster size: {cluster['size']} posts
        - Average engagement: {cluster['avg_engagement']:.2f}
        - Engagement distribution: 
          * High engagement posts: {high_engagement_count} ({high_engagement_count/total_posts*100:.1f}%)
          * Medium engagement posts: {med_engagement_count} ({med_engagement_count/total_posts*100:.1f}%)
          * Low engagement posts: {low_engagement_count} ({low_engagement_count/total_posts*100:.1f}%)
        - Trending score: {cluster['trending_score']:.2f}
        - Recency score: {cluster['recency_score']:.2f}
        
        SAMPLE POSTS:
        
        HIGH ENGAGEMENT POSTS (ðŸ‘ = likes | â†—ï¸ = recasts):
        {chr(10).join(cluster['sample_texts'][:100])}
        
        MEDIUM ENGAGEMENT POSTS:
        {chr(10).join(cluster['sample_texts'][100:150]) if len(cluster['sample_texts']) > 100 else "No medium engagement posts"}
        
        LOW ENGAGEMENT POSTS:
        {chr(10).join(cluster['sample_texts'][150:175]) if len(cluster['sample_texts']) > 150 else "No low engagement posts"}
        
        EXEMPLAR POSTS (closest to cluster center):
        {chr(10).join(cluster['sample_texts'][-25:]) if len(cluster['sample_texts']) > 175 else "No exemplar posts"}
        
        Generate your response based on the following Python TypedDict schema:
        
        class ClusterTopic(TypedDict):
            topic_name: str  # 5 words max
            explanation: str  # Brief explanation of why trending
            estimated_percentage: str  # Percentage of total conversation
            key_terms: list[str]  # List of strings
            engagement_level: str  # High, Medium, Low
            sentiment: str  # Positive, Neutral, Negative, Mixed
            primary_language: str  # English, Spanish, Vietnamese, etc.
            user_demographics: str  # Brief description of who is discussing this topic
            
        IMPORTANT GUIDELINES:
        1. PRIORITIZE TRENDING TOPICS: Focus on identifying what is actively trending, not just popular
        2. RECENT ACTIVITY: Pay attention to timestamps in the posts for recency
        3. ENGAGEMENT MATTERS: Topics with higher likes and recasts are more likely to be trending
        4. SPECIFIC TOPICS: Use concrete, specific names (not generic categories)
        5. LANGUAGE ANALYSIS: Note if the cluster is predominantly in a non-English language
        6. KEY INFLUENCERS: If relevant, identify any key figures driving the conversation
        """
        
        # Get response with JSON formatting
        response = model.generate_content(
            prompt,
            generation_config=types.GenerationConfig(
                temperature=0,
                response_mime_type="application/json"
            )
        )
        
        # Parse JSON response
        try:
            topic_data = json.loads(response.text)
            # Check if topic_data is a list (API might have changed format)
            if isinstance(topic_data, list) and len(topic_data) > 0:
                topic_data = topic_data[0]  # Take the first item if it's a list
            
            # Safely get the topic name with a fallback
            topic_name = "Unknown"
            if isinstance(topic_data, dict) and 'topic_name' in topic_data:
                topic_name = topic_data['topic_name']
                
            print(f"Successfully labeled cluster {cluster['cluster_id']} as '{topic_name}'")
        except json.JSONDecodeError as e:
            print(f"Error parsing JSON for cluster {cluster['cluster_id']}: {e}")
            # Fallback if JSON parsing fails
            topic_data = {
                "topic_name": "Error parsing response",
                "explanation": "Could not parse JSON from Gemini response",
                "estimated_percentage": "unknown",
                "key_terms": cluster['keywords'][:5],
                "engagement_level": "unknown",
                "sentiment": "unknown"
            }
        
        # Add to results
        cluster_topics.append({
            'cluster_id': cluster['cluster_id'],
            'size': cluster['size'],
            'total_engagement': float(cluster['total_engagement']),
            'avg_engagement': float(cluster['avg_engagement']),
            'topic_data': topic_data,
            'keywords': cluster['keywords']
        })
    
    # Save intermediate results
    with open('output/approach2_results.json', 'w') as f:
        json.dump(cluster_topics, f, indent=2)
    
    print(f"LDA + K-Means approach completed in {time.time() - start_time:.2f} seconds")
    
    return cluster_topics

if __name__ == "__main__":
    # This module is imported and run from the main.py file
    pass